# üõ°Ô∏è Sistema ETL de Migraci√≥n y Enmascaramiento de Datos

**FES Acatl√°n - Matem√°ticas Aplicadas y Computaci√≥n**
**Materia:** Administraci√≥n de Bases de Datos

Este proyecto implementa un pipeline **ETL (Extract, Transform, Load)** robusto desarrollado en Python. Su objetivo principal es migrar datos sensibles desde un entorno de Producci√≥n a un entorno de QA (Calidad), aplicando reglas de enmascaramiento en tiempo real para proteger la Informaci√≥n Personal Identificable (PII).

El sistema cuenta con una interfaz gr√°fica interactiva construida con **Streamlit**, gesti√≥n de estado local, logs de auditor√≠a en base de datos y soporte para Docker.

---

## üìã Caracter√≠sticas Principales

* **Idempotencia y Persistencia:** Capacidad de reanudar cargas interrumpidas gracias al archivo de estado (`state.json`).
* **Modos de Ejecuci√≥n:**
    * üî¥ **Carga Completa (Full Load):** Limpieza total del destino y recarga desde cero.
    * üîÑ **Carga Incremental (Delta):** Detecci√≥n y migraci√≥n solo de registros nuevos.
    * üß™ **Modo Ensayo (Dry Run):** Verificaci√≥n de conexiones sin alterar datos.
* **Enmascaramiento de Datos:** Transformaci√≥n irreversible de datos sensibles (Email, Tarjetas, Nombres).
* **Auditor√≠a Dual:** Logs t√©cnicos en archivo local JSON y logs de cumplimiento en tabla SQL.
* **Monitor en Tiempo Real:** Visualizaci√≥n inmediata de los datos migrados en la interfaz.

---

## üõ†Ô∏è a. Pasos de Instalaci√≥n (Setup)

### 1. Requisitos Previos
* **Python 3.9** o superior.
* **PostgreSQL** (Local o Supabase).
* **Docker Desktop** (Opcional, si se desea contenedorizar se recomienda esta opci√≥n como prioridad).
Esta es la fomra recomendada para mover el ETL a QA o Producci√≥n, ya que esto nos garantiza que tengamos un entorno inmutable.
Asegura que el entorno de ejecuci√≥n sea identico en cualquier servidor.

### Instlaci√≥n de docker
Antes de cualquier cosa instalaremos docker desde internet en la siguiente liga https://www.google.com/search?client=opera&q=docker&sourceid=opera&ie=UTF-8&oe=UTF-8&sei=p1A7aZLoEaGnqtsP54vf2As
para usar el docker que necesitaremos mas adaelante.

Empecemos con lo primeero que tenemos que crear una carpeta donde tenemos que clonar algunos archivos especificos de nuestro repositorio de github, los cuales son:
### 1. app.py
### 2. config.yaml
### 3. dockerfile
### 4. generar_datos.py
### 5. main.py
### 6. prueba.py.
### 7. requirements.txt 

Esto se puede clonar con un comando estandar en la cmd para pasar los archivos del repositorio a la carpeta creada anteriormente:

git clone [URL_DEL_REPOSITORIO] [NOMBRE_DE_LA CARPETA]

Buscamos en nuestro explorador de archivos la carpeta en la cual clonamos los archivos de nuestro repositorio de github y la seleccionamos, dentro de ella en la aparte de arriba debe decir algo
como esto "D:\proyecto_abd" (donde se encuentra nuestra carpeta en el ordenador) borramos eso y ponemos cmd + enter; enseguida nos desplegara la terminal y dira algo como esto
"D:\proyecto_abd>".

Despu√©s construimos la imagen del docker con el siguiente comando:

### D:\proyecto_abd>docker build -t proyecto_abd

Ahora ejecutamos la imagen que acabamos de crear.

### D:\proyecto_abd>docker run -d -p 8501:8501 --name etl-final proyecto_abd

Ahora verificamos el estado del contenedor

### D:\proyecto_abd>docker ps

Accedemos a la interfaz gr√°fica (Streamlit).
Abre tu navegador web y ve a la sigueinte direcci√≥n: https://localhost:8501.
A partir de aqu√≠, el profesor puede verificar todos los requisitos desde la interfaz:

1. Login (Requisito 8: RBAC): Iniciar sesi√≥n como dev con la contrase√±a ABD123.

2. Generaci√≥n de Datos (Requisito 4): Usar el bot√≥n de generar datos para poblar la tabla fuente.

3. Full Load: Ejecutar la carga completa (verifica Rendimiento/Batch Insert).

4. Delta Load: Ejecutar la carga incremental (verifica Requisito 3).

5. Inspector: Verificar que los datos en DESTINO est√°n enmascarados (Requisito 2).

6. Auditor√≠a: Verificar que los logs se guardan correctamente (Requisito 5 y 6).








# üõ°Ô∏è b. Manejo de Seguridad

La arquitectura de seguridad del proyecto se basa en tres pilares para cumplir con los requerimientos acad√©micos y las mejores pr√°cticas de ingenier√≠a de datos:

### 1. Almacenamiento de Credenciales (Segregaci√≥n)
* **Separaci√≥n de C√≥digo y Configuraci√≥n:** Las credenciales de base de datos **NO** est√°n expuestas dentro del c√≥digo fuente (`main.py` o `app.py`).
* **Archivo de Configuraci√≥n Excluido:** Se utiliza un archivo `config.yaml` externo para la conexi√≥n. Este archivo est√° incluido en el `.gitignore`, asegurando que **nunca se suba al repositorio p√∫blico**.
* **Plantilla Segura:** En el repositorio se incluye un archivo `config.example.yaml` con credenciales ficticias para guiar la instalaci√≥n sin comprometer la seguridad.

### 2. Control de Acceso Basado en Roles (RBAC)
El sistema implementa una capa de autenticaci√≥n simulada en el Frontend (`app.py`) para restringir operaciones cr√≠ticas seg√∫n el perfil del usuario:

| Rol | Permisos | Contrase√±a (Demo) |
| :--- | :--- | :--- |
| **Invitado** | Solo lectura (Ver gr√°ficas y logs de auditor√≠a). | *N/A* |
| **Operador** | Ejecuci√≥n de Carga Incremental (Delta) y Modo Ensayo. | `ABD123` |
| **Dev (Admin)** | Control Total, incluyendo Carga Completa (Truncate) y reinicio de estado. | `ABD123` |

### 3. Trazabilidad y Auditor√≠a
Cada operaci√≥n genera un `execution_id` √∫nico (UUID) que garantiza el no repudio de las acciones. Este ID queda registrado en una **doble bit√°cora**:
* **Local:** Archivo `logs_historial.json` para consulta inmediata.
* **Remota:** Tabla inmutable `auditoria_logs` en PostgreSQL para an√°lisis forense.
